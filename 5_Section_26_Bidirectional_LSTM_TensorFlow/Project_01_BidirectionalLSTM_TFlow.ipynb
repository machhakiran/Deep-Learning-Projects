{"cells":[{"cell_type":"code","execution_count":10,"id":"32edd762","metadata":{"id":"32edd762","executionInfo":{"status":"ok","timestamp":1718732071295,"user_tz":240,"elapsed":211,"user":{"displayName":"zeeshan ahmad","userId":"09790783261567949831"}}},"outputs":[],"source":["from keras.models import Model\n","from keras.layers import Input, LSTM, Bidirectional\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["# 1-When retrun_sequences = True, return_state = False"],"metadata":{"id":"94ILXVrfFUjc"},"id":"94ILXVrfFUjc"},{"cell_type":"code","source":["T = 4  # time steps\n","D = 2 # features\n","H = 3 # LSTM Units"],"metadata":{"id":"Hu397VapFmlS","executionInfo":{"status":"ok","timestamp":1718732119990,"user_tz":240,"elapsed":219,"user":{"displayName":"zeeshan ahmad","userId":"09790783261567949831"}}},"id":"Hu397VapFmlS","execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### Create the input data"],"metadata":{"id":"znMv9BqbFp0r"},"id":"znMv9BqbFp0r"},{"cell_type":"code","source":["X = np.random.randn(1, T, D)\n","print(\"Input shape\", X.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"91lnzuK3FzET","executionInfo":{"status":"ok","timestamp":1718732124646,"user_tz":240,"elapsed":194,"user":{"displayName":"zeeshan ahmad","userId":"09790783261567949831"}},"outputId":"e3729a22-40aa-44bc-8925-612e1ea5cfff"},"id":"91lnzuK3FzET","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Input shape (1, 4, 2)\n"]}]},{"cell_type":"markdown","source":["### Create LSTM Model"],"metadata":{"id":"NJc1sbvWF5CL"},"id":"NJc1sbvWF5CL"},{"cell_type":"code","source":["input = Input(shape=(T, D))\n","Bi_LSTM = Bidirectional(LSTM(H, return_sequences = True,  return_state = False,))\n","\n","x = Bi_LSTM(input)\n","\n","model1 = Model(inputs=input, outputs=x)"],"metadata":{"id":"Fei6czzdF8sT","executionInfo":{"status":"ok","timestamp":1718732186594,"user_tz":240,"elapsed":840,"user":{"displayName":"zeeshan ahmad","userId":"09790783261567949831"}}},"id":"Fei6czzdF8sT","execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["### Get the output from the model"],"metadata":{"id":"udu-1dnWGJyr"},"id":"udu-1dnWGJyr"},{"cell_type":"code","source":["output = model1.predict(X)\n","print(\"\\nOutput.shape is T x 2H :\", output.shape) # T X 2H\n","print(\"\\n\\nOutput represents the hidden states of all the time steps\")\n","print(\"\\n The hidden state of the bidirectional LSTM is the concatenation of forward and reverse LSTM\")\n","print(\"\\nOutput:\", output)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P2fm0bsnGQOj","executionInfo":{"status":"ok","timestamp":1718732207169,"user_tz":240,"elapsed":1601,"user":{"displayName":"zeeshan ahmad","userId":"09790783261567949831"}},"outputId":"1a327499-b36d-4be5-8f3b-780851f9d8b3"},"id":"P2fm0bsnGQOj","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 1s 1s/step\n","\n","Output.shape is T x 2H : (1, 4, 6)\n","\n","\n","Output represents the hidden states of all the time steps\n","\n"," The hidden state of the bidirectional LSTM is the concatenation of forward and reverse LSTM\n","\n","Output: [[[ 0.07456203  0.00503339 -0.0489476  -0.12916952 -0.25863355\n","   -0.06716065]\n","  [ 0.06046013 -0.09754155 -0.18069823 -0.07110782 -0.33665645\n","   -0.06914528]\n","  [ 0.26672763  0.03998921 -0.28739437 -0.05359246 -0.10900691\n","   -0.20306058]\n","  [-0.02198117 -0.05483439 -0.0770731   0.16711795  0.00506367\n","    0.08505351]]]\n"]}]},{"cell_type":"markdown","id":"578bf517","metadata":{"id":"578bf517"},"source":["# 2-When return_sequences = False, return_state = True"]},{"cell_type":"markdown","source":["### Create LSTM Model"],"metadata":{"id":"G2S25kCfB6jS"},"id":"G2S25kCfB6jS"},{"cell_type":"code","source":["input = Input(shape=(T, D))\n","Bi_LSTM = Bidirectional(LSTM(H, return_sequences = False, return_state = True))\n","\n","x = Bi_LSTM(input)\n","\n","model2 = Model(inputs=input, outputs=x)"],"metadata":{"id":"dIUsPIoGBqpi","executionInfo":{"status":"ok","timestamp":1718732337986,"user_tz":240,"elapsed":756,"user":{"displayName":"zeeshan ahmad","userId":"09790783261567949831"}}},"id":"dIUsPIoGBqpi","execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["### Get the output from the model"],"metadata":{"id":"K1IQytDDBekT"},"id":"K1IQytDDBekT"},{"cell_type":"code","execution_count":16,"id":"4116424f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4116424f","executionInfo":{"status":"ok","timestamp":1718732349933,"user_tz":240,"elapsed":1029,"user":{"displayName":"zeeshan ahmad","userId":"09790783261567949831"}},"outputId":"f7f22a08-16b0-41a2-e19f-0c364ce6e235"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7e098f794430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 1s 785ms/step\n","\n","Output: [[-0.34828168 -0.07542712  0.18684222  0.12340843 -0.14375435 -0.01171371]]\n","\n","Output.shape: (1, 6)\n","\n","h1: [[-0.34828168 -0.07542712  0.18684222]]\n","\n","c1: [[-0.79326814 -0.20503357  0.3167442 ]]\n","\n","h2: [[ 0.12340843 -0.14375435 -0.01171371]]\n","\n","c2: [[ 0.26265776 -0.3506873  -0.01710619]]\n"]}],"source":["output, h1, c1, h2, c2 = model2.predict(X)\n","print(\"\\nOutput:\", output) # Concatenation of both hidden states 1.e forward and reverse.\n","print(\"\\nOutput.shape:\", output.shape)\n","print(\"\\nh1:\", h1)\n","print(\"\\nc1:\", c1)\n","print(\"\\nh2:\", h2)\n","print(\"\\nc2:\", c2)"]},{"cell_type":"code","execution_count":null,"id":"b23fddf3","metadata":{"id":"b23fddf3"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"608bba0a","metadata":{"id":"608bba0a"},"source":["# 3-When return_sequences = True, return_state = True"]},{"cell_type":"markdown","source":["### Create LSTM"],"metadata":{"id":"4O2OQgdfCjKD"},"id":"4O2OQgdfCjKD"},{"cell_type":"code","execution_count":17,"id":"425f8d80","metadata":{"id":"425f8d80","executionInfo":{"status":"ok","timestamp":1718732479756,"user_tz":240,"elapsed":774,"user":{"displayName":"zeeshan ahmad","userId":"09790783261567949831"}}},"outputs":[],"source":["input = Input(shape=(T, D))\n","Bi_LSTM = Bidirectional(LSTM(H, return_state = True, return_sequences = True))\n","\n","x = Bi_LSTM(input)\n","\n","model3 = Model(inputs=input, outputs=x)"]},{"cell_type":"markdown","source":["### Get the output from the LSTM"],"metadata":{"id":"QfOsG-IvCocq"},"id":"QfOsG-IvCocq"},{"cell_type":"code","execution_count":18,"id":"380a67d6","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"380a67d6","executionInfo":{"status":"ok","timestamp":1718732483028,"user_tz":240,"elapsed":1095,"user":{"displayName":"zeeshan ahmad","userId":"09790783261567949831"}},"outputId":"45d4654e-2a25-4842-d296-f71ecfeefe44"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7e09942e85e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 1s 827ms/step\n","\n","Output: [[[ 0.02446933 -0.04255127  0.05460095  0.29228726  0.06640132\n","    0.08378489]\n","  [ 0.07289644 -0.10540698  0.05403689  0.3524019   0.03364695\n","    0.06473248]\n","  [ 0.05808809 -0.12808125  0.13985644  0.12674901  0.00670078\n","    0.03838923]\n","  [ 0.03870293 -0.27551463  0.08841506 -0.08945619  0.08415356\n","   -0.11686565]]]\n","\n","h1: [[ 0.03870293 -0.27551463  0.08841506]]\n","\n","c1: [[ 0.11600985 -0.4863562   0.15387408]]\n","\n","h2: [[0.29228726 0.06640132 0.08378489]]\n","c2: [[0.83163214 0.17187534 0.16580516]]\n"]}],"source":["output, h1, c1, h2, c2 = model3.predict(X)\n","print(\"\\nOutput:\", output)\n","print(\"\\nh1:\", h1)\n","print(\"\\nc1:\", c1)\n","print(\"\\nh2:\", h2) # h2 is the final hidden state for the backward LSTM. So going from front to back the final hidden state for\n","                 # backward LSTM should be at the front of the sequence\n","print(\"c2:\", c2)"]},{"cell_type":"code","execution_count":null,"id":"40f1f2b1","metadata":{"id":"40f1f2b1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"d8872fcf","metadata":{"id":"d8872fcf"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"3f7d6174","metadata":{"id":"3f7d6174"},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}